# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

## Week 1 - Practical aspects of Deep Learning

### Setting up your ML application

#### Train/Dev/Test sets

**Ratios**

Usually: 70/30 or 60/20/20

But with DL usually data set are larger. For instance with 1M examples, could be 98/1/1, or even 99.5/0.4/0.1


**Mismatched train/test distribution**

Make sure examples come from the same distribution. For instance issues can occur training on images taken from website versus photos from smartphone camera.

**No test sets**

Can be OK to have only a dev set

#### Bias/Variance

Compare train and dev set errors, taking into account the optimal (Bayes) error.

#### Basic recipe for ML

**High Bias**:

* Bigger Networks
* Train longer
* (different NN architecture)

**High variance**:

* More data
* Regularization
* (different NN architecture)




### Regularizing your NN

#### Dropout regularization

#### Other regularizations

### Setting up your optimiation problem

#### Normalizing inputs

#### Vanishing/Exploding gradients

#### Weight init in DNN

#### Numerical approximation of gradients

#### Gradient checking

## Week 2 - Optimization algorithms

### 

## Week 3 - Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

### 
